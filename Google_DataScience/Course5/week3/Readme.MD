# Glossary terms from week 3

### Terms and definitions from Course 5, Week 3

**Adjusted R2:** A variation of R2 that accounts for having multiple independent variables present in a linear regression model										

**Backward elimination:** A stepwise variable selection process that begins with the full model, with all possible independent variables, and removes the independent variable that adds the least explanatory power to the model								

**Bias:** Refers to simplifying the model predictions by making assumptions about the variable relationships	

**Bias-variance trade-off:** Balance between two model qualities, bias and variance, to minimize overall error for unobserved data									

**Errors:** The natural noise assumed to be in a regression model			

**Extra Sum of Squares F-test:** Quantifies the difference between the amount of variance that is left unexplained by a reduced model that is explained by the full model				

**Feature selection:** (Refer to variable selection)							

**Forward selection:** A stepwise variable selection process that begins with the null mode—with 0 independent variables—which considers all possible variables to add; it incorporates the independent variable that contributes the most explanatory power to the model			

**Homoscedasticity assumption:** An assumption of simple linear regression stating that the variation of the residuals (errors) is constant or similar across the model				

**Independent observation assumption:** An assumption of simple linear regression stating that each observation in the dataset is independent						

**Interaction term:** Represents how the relationship between two independent variables is associated with changes in the mean of the dependent variable					

**Linearity assumption:** An assumption of simple linear regression stating that each predictor variable (Xi) is linearly related to the outcome variable (Y)						

**Multiple linear regression:** A technique that estimates the relationship between one continuous dependent variable and two or more independent variables				

**Multiple regression:** (Refer to multiple linear regression)					

**No multicollinearity assumption:** An assumption of simple linear regression stating that no two independent variables (Xi and Xj) can be highly correlated with each other			

**Normality assumption:** An assumption of simple linear regression stating that the residuals are normally distributed										

**One hot encoding:** A data transformation technique that turns one categorical variable into several binary variables										

**Overfitting:** When a model fits the observed or training data too specifically and is unable to generate suitable estimates for the general population						

**R2 (The Coefficient of Determination):** The proportion of variance of the dependent variable, Y, explained by the independent variable or variables, X						

**Regularization:** A set of regression techniques that shrinks regression coefficient estimates towards zero, adding in bias, to reduce variance							

**Variable selection:** The process of determining which variables or features to include in a given model												

**Variance:** Refers to model flexibility and complexity, so the model learns from existing data

**Variance inflation factors (VIF):** Quantifies how correlated each independent variable is with all of the other independent variables								
